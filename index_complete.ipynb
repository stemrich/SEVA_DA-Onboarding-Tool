{
 "cells": [
  {
   "source": [
    "# Data Analytics Onboarding Tool\n",
    "\n",
    "Diesem Python-Notebook wurde entwickelt um\n",
    "\n",
    "- eine weitgehend automatische deskriptive Analyse von Datensätzen zu ermöglichen\n",
    "- die Potentiale von code-basierter Analyse zu zeigen\n",
    "- einen Ausblick auf die Möglichkeiten von Machine-Learning zu geben\n",
    "\n",
    "Beispielhaft wird hier der Gemeindedatensatz aus der DDJ-Lehrveranstaltung der FH Joanneum Graz verwendet.\n",
    "\n",
    "___\n",
    "\n",
    "### Hinweise zur Benützung des Notebooks\n",
    "\n",
    "* Das Tools selbst ist interaktiv und somit ist es Möglich den Programmiercode an jeder Stelle zu verändern und an persönliche Bedürfnisse anzupassen.\n",
    "\n",
    "* Das originale Notebook wird dabei nicht überschrieben. Änderungen sind daher unbedenklich - das Original kann jederzeit wiederhergestellt werden.\n",
    "\n",
    "* Bei längerer Inaktivität wird deine Session beendet. Alle Änderungen gehen dann auch verloren. Du kannst aber deine Änderungen im Browser speichern und später wieder laden (mit den zwei Wolken-Icons).\n",
    "![Screenshot Zelle ausführen](screenshots/save-restore.jpg)\n",
    "\n",
    "* Das Notebook besteht aus separaten Zellen.\n",
    "\n",
    "* Die aktive Zelle ist durch einen blauen vertikalen Balken am linken Rand gekennzeichnet.\n",
    "\n",
    "* Es wird unterschieden zwischen Text- und Code-Zellen.\n",
    "\n",
    "* In Text-Zellen, so wie diese, finden sich Erklärungen und Anleitungen.\n",
    "\n",
    "* In Code-Zellen steht der ausführbare Programmiercode.\n",
    "\n",
    "* Du kannst mit “Enter” oder Doppelklick die aktive Zelle bearbeiten, also den Text oder Code in der Zelle ändern. Dann ändert sich der vertikale Balken links von blau auf grün.\n",
    "\n",
    "* In Code-Zellen dient die Raute **#** zum Auskommentieren. Solche Zeilen können entweder Erklärungen sein, oder dazu dienen Optionen im Code auszuprobieren (indem eine Zeile auskommentiert und/oder eine andere aktiviert wird).\n",
    "\n",
    "* Mit \"Strg+Enter\" oder dem Button \"Run\" wird die aktive Zelle ausgeführt und danach die nächste Zelle aktiviert.\n",
    "![Screenshot Zelle ausführen](screenshots/da-firstview2.jpg)\n",
    "\n",
    "* Handelt es sich um eine Text-Zelle (wie diese hier), geschieht dabei weiter nichts.\n",
    "\n",
    "* Wenn es eine Code-Zelle ist, dann wird der Code der Zelle ausgeführt und ein entsprechendes Ergebnis ausgegeben. Eine Code-Zelle ist daran erkennbar, dass die Formatierung deutlich anders aussieht und links von ihr dieses Symbol zu finden ist:\n",
    "> In \\[#\\]:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Datenimport\n",
    "\n",
    "Im ersten Schritt werden die notwendigen Libraries, sowie der Datensatz selbst geladen.\n",
    "\n",
    "* **pandas** ist eine sehr mächtige Library die speziell für Datenanalyse entwickelt wurde. Die Library verfügt auch über eine Vielzahl an Datenimport (und -Export) Befehlen.\n",
    "* **seaborn** ist eine Library für die statistische Datenvisualisierung.\n",
    "\n",
    "**import pandas as pd** importiert die gesamte *pandas*-Library und stellt sie unter dem Kürzel *pd* zur Verfügung. Somit können im Folgenden befehle aus der Library mit *pd.gewuenschter_befehl* abgerufen werden. Wie etwa der Befehl *read_csv* der den Datenimport aus CSV-Dateien erledigt.\n",
    "\n",
    "___\n",
    "\n",
    "### Coding\n",
    "Wenn du Gefallen am Programmieren findest, dir aber weitere Befehle \"fehlen\" (was auch erfahrenen ProgrammiererInnen regelmäßig passiert), dann sind das einige der ersten Anlaufstationen für Hilfe:\n",
    "\n",
    "* Eine Sammlung von [grundlegenden Python Befehlen ist hier zu finden](https://www.pythoncheatsheet.org/)\n",
    "* Für Python Libraries (aber auch jene anderer Programmiersprachen) gibt es eine Vielzahl an *Cheat Sheets*. Diese beinhalten eine Übersicht über die gängigsten Befehle der Library.\n",
    "* Ein Cheat-Sheet für die Pandas Library [findest du hier](https://intellipaat.com/mediaFiles/2018/12/Python-Pandas-Cheat-Sheet.png)\n",
    "* Einfach danach suchen (via Quant, DuckDuckGoGo, Google, ...). Bei Befehlen von Libraries diese immer mit angeben. Z.B.: *python pandas show row with maximum*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Import kompletter Libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Import einer einzigen Funktion aus einer Library\n",
    "from pandas_profiling import ProfileReport\n",
    "from profilehilfe import profile_helfen # Erweiterte DA-Hilfestellung\n",
    "\n",
    "# Import des Gemeindedatensatzes mit Pandas \n",
    "# die Option sep=\";\" gibt das Trennzeichen für die Spalten an (separator)\n",
    "gemeindedaten = pd.read_csv('alle_gemeinden.csv', sep=';')\n",
    "\n",
    "# Ausgabe der ersten Zeilen zur Kontrolle\n",
    "gemeindedaten.head()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Was hier auffällt ist, dass im CSV-File Spalten sind in denen ein \",\" als Dezimaltrennzeichen verwendet wird, und solche in denen ein \".\" verwendet wird. Dieses Problem ist ein extrem gängiges Problem, welches auch bei der Verwendung von Excel (oder anderen Tabellenkalkulationsprogrammen) berücksichtig werden muss.\n",
    "\n",
    "Durch die unterschiedlichen Dezimaltennzeichen sind die Daten nicht vernünftig verarbeitbar, da sie falsch eingelesen werden: Der Befehl *.dtypes* wird auf unseren Datensatz angewandt und zeigt uns, als was für Daten die einzelnen Spalten erkannt wurden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gemeindedaten.dtypes)"
   ]
  },
  {
   "source": [
    "Wie man sieht, sind die Spalten \"Name\", \"Alter\", \"Grundstückspreise\" und \"Anzahl der Kinder\" als *object* kodiert - also keine Zahlen (float oder integer).\n",
    "\n",
    "Um dieses Problem zu lösen werden wir die Dezimaltrennzeichen im CSV-File vereinheitlichen (auf \".\"). Dazu gehen wir wie folgt vor:\n",
    "\n",
    "1. Einlesen des CSV\n",
    "2. Umwandeln aller \",\" die eventuell als Text vorhanden sind durch eine sehr unwahrscheinliche Zeichenkombination (##@#!)\n",
    "3. Ersetzen aller vorhandener \",\" durch \".\"\n",
    "4. Rückumwandlung der Beistriche"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Öffnen des CSV-Files mit den Daten\n",
    "with open(\"alle_gemeinden.csv\") as datei:\n",
    "    content = datei.read()\n",
    "\n",
    "# Vereinheitlichen der Dezimaltrennzeichen\n",
    "content = content.replace('\",\"','##@#!')    # Umwandlung von notwendigen Beistriche in Text\n",
    "content = content.replace(',','.')          # Ersetzen aller (verbliebenen) Kommas durch Punkt\n",
    "content = content.replace('##@#!','\",\"')    # Rückumwandlung der Text-Beistriche\n",
    "\n",
    "# Speichern des (neuen) CSV-Files\n",
    "with open(\"gemeinden.csv\", \"w\") as datei:\n",
    "    content = datei.write(content)\n",
    "\n",
    "# Import des CSV-Files mithilfe des Pandas-Library Befeheles \"read_csv\", welcher\n",
    "# einen sogenannten \"Pandas DataFrame\" anlegt. In diesem Format wird die \n",
    "# Variable \"gemeindedaten\" gespeichert.\n",
    "gemeindedaten = pd.read_csv('gemeinden.csv', sep=';', na_values=[\"-\"])\n",
    "\n",
    "# Kontrolle anhand der ersten Zeilen\n",
    "gemeindedaten.head()"
   ]
  },
  {
   "source": [
    "Nun sind alle Spalten korrekt kodiert, was man sich mit *gemeindedaten.dtypes* auch noch ansehen kann (Kommentar vor dem Befehl entfernen).\n",
    "\n",
    "Bleibt noch zu klären, ob alle Daten eingelesen wurden. Mithilfe von *shape* wird die Dimension des Datensatzes ausgegeben."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wiederholte Kontrolle des Datensatzes:\n",
    "# kommentiere die Folgende Zeile aus um sie ausführen zu können. \n",
    "\n",
    "#gemeindedaten.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfe ob Datensatz vollständig ist (auskommentieren)\n",
    "# Ausgabe erfolgt im Format \n",
    "# (Anzahl Zeilen, Anzahl Spalten)\n",
    "\n",
    "#gemeindedaten.shape"
   ]
  },
  {
   "source": [
    "Ist der Datensatz vollständig?\n",
    "Laut [Wikipedia](https://de.wikipedia.org/wiki/Gemeinde_(%C3%96sterreich)) gibt es in Österreich 2095 Gemeinden (Stand 1. Jänner 2020)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Deskriptive Analyse\n",
    "\n",
    "Der Datensatz soll nun einer ersten Analyse unterzogen werden. Dabei stellt sich die Frage nach der Aussagekraft bei der Gegenüberstellung der einzelnen Gemeinden. Nachdem eine Großstadt mehr Einwohner hat, ist es auch naheliegend, dass sie mehr Ehepaare haben wird als ein kleines Bergdorf. Um die Gemeinden miteinander vergleichen zu können macht es daher für *Anzahl an Ehepaaren* Sinn die Variable in Relation zur Einwohnerzahl zu setzen.\n",
    "\n",
    "Das selbe trifft auch auf *Arbeitsstätten*, *Beschäftigte* und *Erbwerbstätige (15-64)* zu. Diese Spalten werden im Folgenden normiert.\n",
    "___\n",
    "\n",
    "## Normierung"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine Kopie des Datensatzes\n",
    "# (sicher ist sicher - so kann immer auf das Original zugegriffen werden ;)\n",
    "daten_normiert = gemeindedaten.copy()\n",
    "\n",
    "# Division der 4 Spalten/Variablen durch die Einwohneranzahl\n",
    "daten_normiert[\"Arbeitsstätten\"] = daten_normiert[\"Arbeitsstätten\"]/daten_normiert[\"Einwohner\"]\n",
    "daten_normiert[\"Beschäftigte\"] = daten_normiert[\"Beschäftigte\"]/daten_normiert[\"Einwohner\"]\n",
    "daten_normiert[\"Erbwerbstätige (15-64)\"] = daten_normiert[\"Erbwerbstätige (15-64)\"]/daten_normiert[\"Einwohner\"]\n",
    "daten_normiert[\"Anzahl an Ehepaaren\"] = daten_normiert[\"Anzahl an Ehepaaren\"]/daten_normiert[\"Einwohner\"]\n",
    "\n",
    "# Ausgabe zur Kontrolle\n",
    "daten_normiert.head()"
   ]
  },
  {
   "source": [
    "## Deskriptive Analyse\n",
    "\n",
    "Wir können den normierten Datensatz jetzt mit Pandas Profiling analysieren lassen.\n",
    "___\n",
    "\n",
    "Der Report besteht aus 6 Kapiteln.\n",
    "\n",
    "1. *Overview*: Hier werden die Eckdaten angezeit und *Warnings* betreffend die einzelnen Variablen ausgegeben. *Reproduction* ist nicht wirklich relevant.\n",
    "2. *Variables* beinhaltet eine Analyse jeder einzelnen Variablen. Mit *Toggle details* kann jeweils noch ein vielfaches an Zusatzinformationen eingebledet werden.\n",
    "3. *Interactions* bietet die Möglichkeit je 2 Variablen gegeneinander anzuzeigen. Wir werden in einem nächsten Schritt noch eine zweite Möglichkeit dafür kennen lernen.\n",
    "4. *Correlations* berechnet statistische Korrelationsmaße für die Variablen zueinander. Mit *Toggle correlation description* kann man sich eine Beschreibung des jeweiligen Maßes anzeigen lassen.\n",
    "5. *Missing values* weist die Anzahl an fehlenden Werten je Variable aus\n",
    "6. *Sample* ist ein stichprobenartiger Einblick in den Beginn und das Ende unseses Datesatzes (analog zum obigen Befehl *daten_normiert.head()*)\n",
    "\n",
    "___\n",
    "\n",
    "***Hinweis: Das kann bis zu 90 Sekunden dauern!***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling-Analyse unseres Datensatzes\n",
    "profile = ProfileReport(daten_normiert, title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "# Darstellung der Ergebnisse\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "source": [
    "## Fragen und To-Dos\n",
    "\n",
    "Analysiere die Daten mithilfe des *Pandas Profiling Reports* und versuche dabei Fragen zu beantworten:\n",
    "\n",
    "1. Sind die Overview & Warnings für dich verständlich und hilfreich?\n",
    "2. Was bedeuten die Warnings? Sind sie relevant? Wenn ja/nein welche und warum (nicht)?\n",
    "3. Was könnten Gründe für die hohe Korrelation von Variablen sein?\n",
    "4. Gibt es bei den Variablen Probleme?\n",
    "5. Sind die Grafiken verständlich?\n",
    "\n",
    "Wenn du damit fertig bist, führe die folgende Funktion aus, welche dir weitere Hilfestellungen zum Report bietet."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erweiterte automatisierte Info zum Profiling-Report\n",
    "# Import der Funktion erfolgte am Notebook-Beginn.\n",
    "\n",
    "profile_helfen(profile)"
   ]
  },
  {
   "source": [
    "## Vertiefende Erklärungen\n",
    "\n",
    "Helfen dir diese weiterführenden Informationen zum Profiling Report? Bitte erkläre ...\n",
    "\n",
    "* welche dir (nicht) geholfen haben und warum (nicht), sowie\n",
    "* was du zusätzlich zu deiner ersten Analyse herausgefunden/entdeckt hast.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "___"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Ausreißer, Korrelation & Kausalität\n",
    "\n",
    "Das interessante bei Datensätze ist es die \"versteckte\" Information darin zu finden: Zusammenhänge und Muster. Und in weiterer Folge Erklärungen dafür. Im obigen Report haben wir dafür schon *Interactions* und *Correlations* gesehen. Eine weiter Möglichkeit sich einem Datensatz zu nähern bietet der Scatterplot. Genauer gesagt eine Technik die \"Small Multiples\" genannt wird und aus einer Vielzahl an Scatterplots besteht (auch Pairplot genannt).\n",
    "\n",
    "Für den Datensatz lassen wir uns so einen Plot erzeugen.\n",
    "\n",
    "***Hinweis: Das kann bis zu 90 Sekunden dauern!***\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot für visuelle Analyse\n",
    "\n",
    "# Auswahl der Variablen von Interesse\n",
    "# Der Gemeindenamen zB ist hier (als kategorische Variable) nicht sinnvoll\n",
    "# Du kannst die Auswahl nach belieben Ändern!\n",
    "features = [\"Einwohner\", \"Arbeitsstätten\", \"Beschäftigte\", \"Alter\", \"Einkommen\", \"Erbwerbstätige (15-64)\", \"Grundstückspreise\", \"Anzahl an Ehepaaren\", \"Anzahl der Kinder\"]\n",
    "\n",
    "# Erstellen des Plots mit den gewählten Features\n",
    "ft_plot = sns.pairplot(daten_normiert[features])"
   ]
  },
  {
   "source": [
    "Diese Art von Darstellung eignet sich sehr gut um Ausreißer und Zusammenhänge in den Daten zu erkennen.\n",
    "\n",
    "***Ausreißer***\n",
    "\n",
    "Von einem Ausreißer spricht man in der Statistik, wenn ein (Mess-)Wert nicht in eine erwartete Messreihe passt oder allgemein nicht den Erwartungen entspricht.\n",
    "\n",
    "Im Pairplot sind das Punkte die (weit) abseits der (zusammenhängenden) Punktewolken liegen. Ausreißer können unterschiedliche Erklärungen haben. Sie können sowohl auf Datenfehler zurückgehen, als auch völlig legitim sein. Die richtige Einordnung erfordert bei komplexen Daten in der Regel ExpertInnenwissen.\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "## Visuelle Analyse - Ausreißer\n",
    "\n",
    "Analysiere nun den Pairplot (s.a. Logbuch):\n",
    "\n",
    "1. Ist die Visualisierung für dich verständlich?\n",
    "2. Wo fallen dir Ausreißer auf?\n",
    "    * Sind diese in Ordnung?\n",
    "    * Was für Erklärung(en) gibt es dafür?\n",
    "\n",
    "___"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Visuelle Analyse - Zusammenhänge\n",
    "\n",
    "Wenn es in Daten Zusammenhänge gibt, dann kann man diese oft ebenfalls mithilfe von Scatterplots feststellen. Im obigen Beispiel haben wir den (anfangs) normierten Datensatz abgebildet, da sonst der Vergleich einzelner Gemeinden miteinander nicht wirklich aussagekräftig wäre.\n",
    "\n",
    "Um Zusammenhänge zwischen Variablen (Eigenschaften) zu zeigen, eignet sich der ursprüngliche Datensatz jedoch besser.\n",
    "\n",
    "**Aufgabenstellung (s.a. Logbuch):**\n",
    "\n",
    "Erstelle dir einen Pairplot für den Datensatz *gemeindedaten* mit den Variablen\n",
    "\n",
    "* Einwohner\n",
    "* Erbwerbstätige (15-64)\n",
    "* Anzahl an Ehepaaren\n",
    "* Anzahl der Kinder\n",
    "\n",
    "füge ihn als Screenshot ins Logbuch ein und beschreibe was dir dabei (im Gegensatz zum ersten Pairplot) auffällt."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier ist eine Kopie des Befehls von oben als Platzhalter\n",
    "\n",
    "# Auswahl der Variablen von Interesse\n",
    "features = [\"Einwohner\", \"Arbeitsstätten\", \"Beschäftigte\", \"Alter\", \"Einkommen\", \"Erbwerbstätige (15-64)\", \"Grundstückspreise\", \"Anzahl an Ehepaaren\", \"Anzahl der Kinder\"]\n",
    "\n",
    "# Erstellen des Plots mit den gewählten Features\n",
    "ft_plot = sns.pairplot(daten_normiert[features])"
   ]
  },
  {
   "source": [
    "## Korrelation vs. Kausalität\n",
    "\n",
    "Ein gängiger Fehler bei der Datenanalyse ist das Verwechseln von Korrelation und Kausalität.\n",
    "Hängen zwei Merkmale kausal voneinander ab, so ist das eine Merkmal Ursache für Auswirkungen auf das andere.\n",
    "\n",
    "Bei einer Korrelation ist dies nicht der Fall.\n",
    "\n",
    "Ein klassisches Beispiel ist die Anzahl an gegessenem Speiseeis und der Häufigkeit von von Sonnenbränden. In den Sommermonaten steigen beide Merkmale deutlich an. Trotzdem ist eher zweifelhaft, dass der Verzehr von Speiseeis Sonnenbrände verursacht ;)\n",
    "\n",
    "Bevor man von Korrelationen auf Kausalitäten schließt, sollte man beim leisesten Zweifel ExpertInnen des jeweiligen Faches konsultieren.\n",
    "___"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Manuelle Detailanalyse\n",
    "\n",
    "Um die entdeckten Ausreißer überprüfen und dann eiordnen zu können (\"sind sie in Ordnung oder nicht?\"), braucht es Details zu ihnen.\n",
    "\n",
    "Dafür stehen im folgenden 3 Code-Zellen bereit, welche du an deine Bedürfnisse anpassen kannst um die gewünsche Info zu bekommen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe des Datensatzes, sortiert nach einer Spalte\n",
    "# es werden dabei nur die ersten uns letzten 5 Zeilen angezeigt\n",
    "\n",
    "daten_normiert.sort_values('Erbwerbstätige (15-64)', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Gemeinde mit Maximum- oder Minimum-Werten in \n",
    "# einer gegebenen Spalte.\n",
    "\n",
    "# Die Spalte kann beliebige ersetzt werden.\n",
    "# für Maxima nutze den Befehl: argmax()\n",
    "# und argmin() für Minima.\n",
    "\n",
    "daten_normiert.iloc[daten_normiert['Alter'].argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe von Gemeinden mit bestimmtem Namen\n",
    "\n",
    "# Ausgabe der gesäuberten Rohdaten (Datensatz \"gemeindedaten\")\n",
    "# aber Suche in den normierten Daten (\"daten_normiert\").\n",
    "\n",
    "gemeindedaten[daten_normiert['Name']=='Graz']\n",
    "\n",
    "# Erklärung des Befehlsaufbaues:\n",
    "# anzuzeigender_Datensatz[zu_durchsuchender_Datensatz['zu_durchsuchende_spalte']=='zu_suchender_inhalt']\n",
    "\n",
    "# Klarerweise kann auch gemeindedaten[gemeindedaten['Einwohner']==67] gesucht werden.\n",
    "# oder gemeindedaten[gemeindedaten['Einwohner']>=50000] für alle Gemeinden über 50.000 Einwohner"
   ]
  },
  {
   "source": [
    "## Lessons learned\n",
    "\n",
    "Siehe Folien im Logbuch für eine Einordnung der Datenfehler und was man dagegen tun kann.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Datensatz verbessern\n",
    "\n",
    "Dementsprechend wollen wir jetzt unseren Datensatz verbessern, indem wir ihn um eindeutige IDs für die Gemeinden erweitern. So einen Datensatz findet man zum Beispiel auf der Webseite der [Statistik Austria](http://www.statistik.at/web_de/klassifikationen/regionale_gliederungen/gemeinden/index.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen des Datensatzes mit den Gemeindekennzahlen\n",
    "gemeindekennzahlen = pd.read_csv('gemliste_knz.csv', sep=';')\n",
    "\n",
    "# Visuelle Kontrolle des Inhaltes\n",
    "gemeindekennzahlen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sowie Kontrolle ob die Spalten korrekt erkannt worden sind\n",
    "gemeindekennzahlen.dtypes"
   ]
  },
  {
   "source": [
    "Inhalt und Datentypen scheinen zu passen.\n",
    "\n",
    "* *Status* beschreibt ob eine Gemeinde Stadtrang oder Marktrecht hat\n",
    "* *PLZ* und *weitere Polstleitzahlen* sind für uns nicht relevant.\n",
    "* *Gemeindekennziffer* und *Gemeindecode* unterscheiden sich nur für die Wiener Bezirke: hier ist die Gemeindekennziffer für alle Bezirke gleich, während der Code eindeutig ist (wie der nächste Code-Block zeigt). Für uns ist also der Code interessanter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die 23 Wiener Gemeindebezirke sind beginnen mit 9xxxx,\n",
    "# sind also am Ende des Dataframes. Wir können sie daher,\n",
    "# analog zu head() mit tail() ausgeben.\n",
    "\n",
    "gemeindekennzahlen.tail(23)"
   ]
  },
  {
   "source": [
    "Nun können wir ein weitere Stärken von Pandas (und den DataFrames) ausnützen: das Verbinden von Datensätzen!\n",
    "\n",
    "Wir verbinden unseren Gemeindedatensatz (allerdings den normierten *daten_normiert*) mit dem frisch importierten *gemeindekennzahlen* über die Felder *Name* und *Gemeindename*.\n",
    "\n",
    "Der DataFrame-Befehl dazu wird mit *join* aufgerufen\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemeinden_norm_kennz = gemeindekennzahlen.join(daten_normiert.set_index('Name'), on='Gemeindename')\n",
    "\n",
    "gemeinden_norm_kennz.head()"
   ]
  },
  {
   "source": [
    "## Umgang mit Datenfehlern\n",
    "\n",
    "Nach wie vor haben wir das Problem, dass wir die fehlerhaften Zeilen der Doppelgemeinden im Datensatz haben.\n",
    "\n",
    "Dieses Problem ist durch den Befehl *join* sogar noch größer geworden, wie wir uns mit folgendem Befehl ansehen können."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe aller Zeilen im neuen Datensatz gemeinden_norm_kenz\n",
    "# in denen in der Spalte \"Gemeindename\" ein Eintrag aus der \n",
    "# Liste [\"Warth\", \"Mühldorf\", \"Krumbach\"] vorkommt.\n",
    "\n",
    "gemeinden_norm_kennz[gemeinden_norm_kennz.Gemeindename.isin([\"Warth\", \"Mühldorf\", \"Krumbach\"])]"
   ]
  },
  {
   "source": [
    "Statt der tatsächlichen 6 Gemeinden (je 3 mit gleichem Namen) haben wir durch die Verknüpfung mittels *join* eine Verdoppelung.\n",
    "\n",
    "Wie im Logbuch beschrieben, entscheiden wir uns dafür die wenigen (bekannt) fehlerhaften Zeilen zu löschen, denn wir wollen im nächsten Schritt Machine Learning einsetzen. Und diese Technik ist per se nicht komplett genau."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir erstellen eine Kopie unseres bisherigen Datensatzes\n",
    "# \"gemeinden_norm_kennz\" mit dem Namen \"gem_daten_sauber\".\n",
    "gem_daten_sauber = gemeinden_norm_kennz.copy()\n",
    "\n",
    "\n",
    "# Finde alle Zeilen, wo der Inhalte von Spalte \"Gemeindename\" in der Liste\n",
    "# [\"Warth\", \"Mühldorf\", \"Krumbach\"] ist\n",
    "x = gem_daten_sauber.Gemeindename.isin([\"Warth\", \"Mühldorf\", \"Krumbach\"])\n",
    "\n",
    "# ~x ist das Inverse von x. Also alle Zeilen die NICHT in x\n",
    "# (also unsere 3 Gemeinden) sind.\n",
    "# Und nur diese Zeilen wollen wir im sauberen Datensatz gespeichert haben\n",
    "\n",
    "gem_daten_sauber = gem_daten_sauber[~x]\n",
    "\n",
    "gem_daten_sauber.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern des Datensatzes für den Machine-Learning Teil\n",
    "\n",
    "gem_daten_sauber.to_pickle('gem_daten_sauber.pkl')"
   ]
  },
  {
   "source": [
    "# Machine Learning\n",
    "\n",
    "### Was ist Machine Learning?\n",
    "\n",
    "Machine Learning, im Deutschen maschinelles Lernen genannt, ist ein Teilbereich von künstlicher Intelligenz.\n",
    "Ein Machine Learning Algorithmus versucht aus Beispieldaten Zusammenhänge zu finden, womit es dann eine Vorhersage über neue Daten machen kann.\n",
    "Diese Algorithmen sind nicht explizit programmiert, sondern entwickeln sind anhand der Daten ständig weiter.\n",
    "\n",
    "Für eine Vorhersage über neue Daten brauchen wir ein '**target**', also ein Ziel. In unserem Beispiel wird das der **Urbanisierungsgrad** von unseren Gemeinden sein.\n",
    "\n",
    "Es gibt unterschiedliche Arten des maschinellen Lernens. Wir werden uns hier mit **überwachtem Lernen** beschäftigen, wo wir unseren Algorithmen die 'richtige Lösung' bei den Beispieldaten verraten.\n",
    "Weiters unterscheidet man zwischen **Klassifikation** und **Regression**. Bei einer Klassifikation wollen wir einen kategorischen Wert schätzen, während wir bei der Regression versuchen, so nahe wie möglich auf den richtigen numerischen Wert zu kommen.\n",
    "\n",
    "Da wir den Urbanisierungsgrad unserer Gemeinden vorhersagen wollen und überwachtes Lernen benutzen, müssen wir 'Urbanisierungsgrad' zu unserem Datensatz hinzufügen. Der Urbanisierungsgrad ist mit den Werten 1, 2 und 3 kodiert. Diese repräsentieren nicht numerische, sondern kategorische Werte, weil sie dem Grad der Urbanisierung entsprechen (1: Hoch, 2: Mittel, 3: Niedrig). Wir werden die Werte auf Text umändern, damit klar ist, dass es sich hier um Klassifikation handelt."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Bundesland hinzufügen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idee: ML für Bundesländer: Gemeindekennzahl bzw Gemeindecode ignorieren, \"Bundaesland\" feld erstellen mit erster Ziffer (1-9).\n",
    "#Oder ist es besser, Kategorien zu erstellen? Zahlen 1-9 könnten als numerische Interpretation falsch verwendet werden.\n",
    "\n",
    "gem_daten_sauber['Bundesland'] = gem_daten_sauber['Gemeindekennziffer'].astype(str).str[0]\n",
    "gem_daten_sauber['Bundesland'] = gem_daten_sauber['Bundesland'].astype(int)\n",
    "\n",
    "gem_daten_sauber.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_daten_sauber.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_bundesland = {1:'Burgenland', 2:'Kärnten', 3:'Niederösterreich', 4:'Oberösterreich',\n",
    "                5:'Salzburg', 6:'Steiermark', 7:'Tirol', 8:'Vorarlberg', 9:'Wien'}\n",
    "gem_daten_sauber['Bundesland'].replace(d_bundesland, inplace=True)"
   ]
  },
  {
   "source": [
    "### Urbanisierungsgrad hinzufügen\n",
    "\n",
    "Quelle: [Statistik Austria](https://www.statistik.at/web_de/klassifikationen/regionale_gliederungen/stadt_land/index.html)\n",
    "\n",
    "Unser neuer Datensatz besteht aus 3 den Spalten *GKZ*, *CODE* und *TXT.\n",
    "\n",
    "* Über die Spalte *GKZ* können wir diesen Datensatz mit unseren sauberen Gemeindedaten (via *Gemeindekennziffer*) verbinden.\n",
    "* Die Spalte *TXT*, wo die Kodierung in Textform gespeichert ist, werden wir nicht brauchen,\n",
    "* weil die numerische Kodierung index *CODE* in Text umwandeln werden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urbanisierungsgrad = pd.read_csv('gemeinden_urbanisierungsgrad.csv', sep=';')\n",
    "\n",
    "urbanisierungsgrad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenführen der beiden Datensätze über die Gemeindekennziffern\n",
    "\n",
    "gem_daten_sauber = gem_daten_sauber.join(urbanisierungsgrad.set_index('GKZ'), on='Gemeindekennziffer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuelle Überprüfung des neuen, kombinierter Datensatzes:\n",
    "gem_daten_sauber.head()"
   ]
  },
  {
   "source": [
    "***Achtung:*** Bei manchen Codezellen muss man aufpassen, dass man sie nicht mehrmals ausführt. In der folgenden Codezelle werden nämlich Spalten aus dem Datensatz entfernt. Führt man diese Zelle nochmal aus, kommt eine Fehlermeldung, dass die Spalten, die man versucht hat zu entfernen, nicht gefunden wurden. Das passiert, weil diese Spalten schon bei der ersten Ausführung von der Codezelle entfernt wurden und nun nicht mehr existieren. Falls du auf so ein Problem stoßt, kannst du einfach alle Codezellen von oben neu ausführen."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernung von Spalten die wir (fürs Machine-Learning) \n",
    "# nicht mehr brauchen:\n",
    "df_norm.drop(columns=['PLZ des Gem.Amtes', 'weitere Postleitzahlen', 'TXT'], inplace=True)\n",
    "\n",
    "# Die Spalte 'CODE' nennen wir nun 'Urbanisierungsgrad'\n",
    "df_norm.rename(columns= {'CODE' : 'Urbanisierungsgrad'}, inplace=True)\n",
    "\n",
    "# Leserfreundliche Umbenennung der Urbanisierungsgrade:\n",
    "# 1 bedeutet 'Hoch', 2 'Mittel' und 3 ist 'Niedrig'\n",
    "d_urbanisierungsgrad = {1:'Hoch', 2:'Mittel', 3:'Niedrig'}\n",
    "df_norm['Urbanisierungsgrad'].replace(d_urbanisierungsgrad, inplace=True)\n",
    "\n",
    "# Übersicht:\n",
    "df_norm.head()"
   ]
  },
  {
   "source": [
    "Wir wollen unserem Machine Learning Algorithmus also Daten geben, die er noch nicht kennt, damit es auf diesen unbekannten Daten den Urbanisierungsgrad schätzt, welches entweder 'Hoch', 'Mittel' oder 'Niedrig' sein wird.\n",
    "\n",
    "Dafür wollen wir einen kleinen Teil unserer Daten (wir entscheiden uns hier für 2%) raus nehmen und zur Seite stellen, damit dieser am Ende für Schätzungen benutzt werden kann."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2% der Daten ausschließen damit man sie nur zum Schätzen benutzt:\n",
    "ml_datensatz = df_norm.sample(frac=0.98, random_state=101)\n",
    "ungesehene_daten = df_norm.drop(ml_datensatz.index)\n",
    "\n",
    "ml_datensatz.reset_index(drop=True, inplace=True)\n",
    "ungesehene_daten.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Neue Größe unserer Daten: (Anzahl an Zeilen, Anzahl der Spalten)\n",
    "print('Daten für die Modellierung: ' + str(ml_datensatz.shape))\n",
    "print('Ungesehene Daten für Schätzungen: ' + str(ungesehene_daten.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überblick über die Daten, die wir für Machine Learning verwenden:\n",
    "ml_datensatz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überblick über die Daten, die wir zum Schätzen verwenden (42 insgesamt):\n",
    "ungesehene_daten.head()"
   ]
  },
  {
   "source": [
    "# Setup\n",
    "\n",
    "Als ersten Schritt müssen wir '**setup**' ausführen. Gehen wir die wichtigsten Parameter durch: \n",
    "* **data**: Wir müssen den Datensatz, den wir verwenden wollen, angeben.\n",
    "* **target**: Die Spalte, welches wir als Ziel ausgesucht haben und vorhersagen wollen.\n",
    "* **ignore_features**: Spalten die ignoriert werden sollen. Wir entfernen 'Gemeindekennziffer', 'Gemeindename' und 'Gemeindecode', weil diese keine Information über den Urbanisierungsgrad bieten und die Algorithmen potenziell schlechter werden. Zusätzlich ignorieren wir 'Einwohner' und 'Arbeitsstätten', weil diese zu viel über den Urbanisierungsgrad verraten könnten. \n",
    "* **train_size**: Wir unterscheiden zwischen **Trainingsdaten** und **Testdaten**. Trainingsdaten sind die Beispieldaten, die unsere Algorithmen benutzen um zu lernen, während man danach auf den Testdaten die Genauigkeit von den Algorithmen evaluiert. Wir haben uns hier für 70% Trainingsdaten und 30% Testdaten entschieden.\n",
    "\n",
    "Wenn man auf **3: Label Encoded** schaut, sieht man, dass die verschiedenen Urbanisierungsgrade automatisch wieder als Zahlen kodiert wurden.\n",
    "\n",
    "***Hinweis:*** Manche der kommenden Codezellen können einige Minuten dauern!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für Machine Learning benutzen wir PyCaret als Tool und benötigen die Algorithmen für Klassifikation\n",
    "\n",
    "from pycaret.classification import *\n",
    "exp = setup(data=ml_datensatz, target='Urbanisierungsgrad', ignore_features=['Gemeindekennziffer', 'Gemeindename', 'Gemeindecode', 'Einwohner', 'Arbeitsstätten', 'Bundesland'], silent=True, session_id=101, train_size=0.70)"
   ]
  },
  {
   "source": [
    "# Modelltest\n",
    "\n",
    "Als zweiten Schritt wollen wir nun die verschiedenen Machine Learning Algorithmen miteinander vergleichen (**compare_models**). Es werden ein paar von ihnen aufgrund der langen Rechenzeit exkludiert.\n",
    "\n",
    "Die Anzahl an **folds** muss bestimmt werden: 'fold=3' bedeutet, dass wir 3-mal 70% (siehe: setup) unserer Daten als Trainingsdaten verwenden. Dabei werden die Trainingsdaten jedes Mal zufällig ausgesucht und sind daher unterschiedlich.\n",
    "Der Grund, wieso wir folds verwenden, ist damit wir ein sogenanntes **overfitting** vermeiden. Overfitting bedeutet, dass unser Algorithmus die Trainingsdaten zu genau gelernt hat, wodurch Probleme entstehen können, wie zum Beispiel dass es neue Daten komplett falsch schätzt, wenn sie sich zu sehr von den Trainingsdaten unterscheiden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir vergleichen die ML-Algorithmen miteinander:\n",
    "best = compare_models(fold=3, exclude=['lightgbm'])"
   ]
  },
  {
   "source": [
    "Die gelben Markierungen in der Tabelle heben die besten Werte hervor. In diesem Beispiel interessiert uns **Accuracy**, die Genauigkeit vom Algorithmus, am meisten.\n",
    "\n",
    "Wir sehen, dass das *Random Forest* Modell eine sehr hohe Genauigkeit *und* eine niedrige Laufzeit (**TT (Sec)**) hat und wollen den als unser Modell nehmen. Dafür brauchen wir den Befehl *create_model*."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Wähle Random Forest Classifier als Modell, mit 3 folds\n",
    "selected_model = create_model('rf', fold=3)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Mit **tune_model** können wir einige der Ergebnisse weiter verbessern.\n",
    "\n",
    "***Hinweis:*** Die Ausführung dauert länger."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mit tune_model können wir unser ausgesuchtes Modell\n",
    "# (Random Forest Classifier) weiter verbessern.\n",
    "\n",
    "tuned_model = tune_model(selected_model, fold=3)"
   ]
  },
  {
   "source": [
    "## Auswertung der Vorhersagequalität\n",
    "\n",
    "Es existieren viele Visualisierungen, die einem helfen können, die Ergebnisse vom Algorithmus besser zu verstehen. Wir fokussieren uns hier auf drei davon.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "Hier siehst du die Anzahl der Daten, die richtig bzw. falsch klassifiziert wurden. Auf der x-Achse sind die Urbanisierungsgrade, die vom Algorithmus geschätzt wurden, während sich auf der y-Achse die tatsächlichen Urbanisierungsgrade von den Daten befinden.\n",
    "\n",
    "**Errinnerung:** Urbanisierungsgrad: 'Hoch' wurde als '0' kodiert, 'Mittel' mit 1 und 'Niedrig' mit 2.\n",
    "\n",
    "Vielleicht hast du bemerkt, dass die Anzahl der Daten, die hier vorkommen, weniger ist als die Größe deines Datensatzes. Das liegt daran, dass nur 30% der Daten als Testdaten verwendet werden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plot_model(tuned_model, plot='confusion_matrix')"
   ]
  },
  {
   "source": [
    "### Class Prediction Error\n",
    "\n",
    "Das zeigt im Prinzip dieselbe Information wie unsere Confusion Matrix aber in Form eines Balkendiagrammes.\n",
    "\n",
    "Auf der x-Achse ist der echte Urbanisierungsgrad und auf der y-Achse die Anzahl an Schätzungen.\n",
    "\n",
    "Man merkt, dass es sehr wenige Gemeinden mit Urbanisierungsgrad 'Hoch' gibt, sie aber trotzdem alle richtig geschätzt wurden. Ab und zu werden Gemeinden, die Urbanisierungsgrad 'Mittel' oder 'Niedrig' haben, miteinander verwechselt. Wir sehen auch, dass die meisten Gemeinden einen niedrigen Urbanisierungsgrad haben."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fehler bei der Klassifizierung von den unterschiedlichen Urbanisierungsgraden\n",
    "plot_model(tuned_model, plot='error')"
   ]
  },
  {
   "source": [
    "### Feature Importance\n",
    "\n",
    "Beim Feature Importance Plot sieht man, welche Spalten aus unserem Gemeindedatensatz den größten Einfluss zur Bestimmung des Urbanisierungsgrads haben. Gibt es welche, die du nicht erwartet hast?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_model, plot='feature')"
   ]
  },
  {
   "source": [
    "Wien als Bundesland scheint einen auffällig hohen Einfluss auf unser Algorithmus zu haben. Wieso könnte das sein? Am besten, wir schauen uns an, welche Gemeinden denn überhaupt einen hohen Urbanisierungsgrad haben."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir suchen nach allen Datensätzen, die einen hohen Urbanisierungsgrad haben\n",
    "\n",
    "urban_hoch = gem_daten_sauber[gem_daten_sauber['Urbanisierungsgrad'] == 'Hoch']\n",
    "urban_hoch.head(20)"
   ]
  },
  {
   "source": [
    "Es sind fast alle Gemeinden mit hohem Urbanisierungsgrad in Wien. Wenn du Lust hast, kannst du versuchen bei **setup** **Bundesland** zu **exkludieren** und zu schauen, ob sich was an der Genauigkeit (insbesondere bei hoher Urbanisierung) vom Algorithmus ändert. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Abschluss\n",
    "\n",
    "Wenn wir mit unserem Modell zufrieden sind, wollen wir es mit **finalize_model** finalisieren. Es werden jetzt die Testdaten auch zu Trainingsdaten gemacht.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = finalize_model(tuned_model)"
   ]
  },
  {
   "source": [
    "# Neue Daten vorhersagen\n",
    "\n",
    "Wir haben uns am Anfang 2% der Daten herausgenommen, damit wir am Ende mit unserem fertigen Machine Learning Modell ein paar Schätzungen machen können. Mit **predict_model** können wir nun unser Algorithmus auf den ungesehenen Daten verwenden. \n",
    "\n",
    "Du wirst erkennen, dass neben 'Urbanisierungsgrad' eine neue Spalte, nämlich **Label**, dazugekommen ist. Das ist die Klassifizierung vom Algorithmus, während 'Urbanisierungsgrad' der echte Wert von unserer Spalte ist. \n",
    "In Zeile 12 von der Tabelle siehst du beispielsweise, dass der Urbanisierungsgrad von Katzelsdorf eigentlich 'Mittel' ist, aber unser Algorithmus es fälschlicherweise als 'Niedrig' eingeschätzt hat."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geschaetzte_daten = predict_model(final_model, data=ungesehene_daten)\n",
    "geschaetzte_daten.head(42)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da776311d08bb511bbc2463cac11bf655b038c6f06bc1e299f5acf29978e4461"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}